# Searh engine.

## Инфраструктурный репозиторий для приложения поисковый движок.
Другие репозитории проекта:
- Сервис UI https://github.com/Kirill-Garbar/search-engine-ui
- Сервис CRAWLER https://github.com/Kirill-Garbar/search-engine-crawler
- Helm чарты для деплоя приложения в kubernetes https://github.com/Kirill-Garbar/search-engine-deploy

## Требования, которые выставлялись к проекту.
- Автоматизированные процессы создания и управления
платформой
  - Ресурсы GCP
  - Инфраструктура для CI/CD
  - Инфраструктура для сбора обратной связи
- Использование практики IaC (Infrastructure as Code) для
управления конфигурацией и инфраструктурой
- Настроен процесс CI/CD
- Все, что имеет отношение к проекту хранится в Git
- Настроен процесс сбора обратной связи
  - Мониторинг (сбор метрик, алертинг, визуализация)
  - Логирование (опционально)
  - Трейсинг (опционально)
  - ChatOps (опционально)
- Документация
  - README по работе с репозиторием
  - Описание приложения и его архитектуры
  - How to start?
  - CHANGELOG с описанием выполненной работы

## Общее описание.
### Для создания инфры используется terraform. В процессе выполнения создаются следующие ресурсы:
- Kubernetes кластер в GKE. В переменные вынесены конфигурация initial нод и нод большого пула.
- Gitlab сервер. Посчитал необходимым вынести Gitlab из кластера. Интересно было его отдельно интегрировать с кубером.
- Доменные записи в DNS зоне на Gitlab.
- Правила firewall (22 - git, 80 - http, 443 - https).

### Для установки и настройки софта используется ansible и gce dynamic inventory. 
- Плэйбук main.yml состоит из:
  - kubernetes-provision.yml, в котором
    - устанавливается tiller
    - устанавливается инфраструктура из хельм чартов:
      - prometheus с преднастроенными правилами service discovery для элементов приложения search-engine (mongo, rabbit, ui, crawler), элементов самого kubernetes (api-server, nodes), расхода ресурсов подами, ингресс-контроллера.
      - grafana с преднастроенными чартами для мониторинга kubernetes (2 чарта), метрик приложения (параметризовано по релизам), метрик mongo (параметризовано по релизам), метрик rabbitmq
      - EFK: сборный чарт из elasticsearch в единственном экземпляре, fluentd (созданы правила парсинга логов элементов приложения и системных логов kubernetes), kibana
      - ingress-controller в виде daemonSet
    - добавляются DNS имена инфры на полученный IP ингресс-контроллера.
    - добавляются DNS имена на staging и корневое на production.
    - добавляется wildcard запись для review инстансов приложения.
  - gitlab.yml, в котором
    - устанавливается python для работы ansible. Не стал использовать packer, ради упрощения провижнинга.
    - устанавливается gitlab из изменённой роли geerglingguy: изменён gitlab.rb, поменяны местами элементы ради использования всех возможностей конфигурации, удалено создание самоподписанных сертификатов.
 - **провижнинг хельм-чартов сделан через shell, т.к. на момент реалиации проекта не было внятного рабочего модуля для ансибла.**
 - **elastic так же как и гитлаб я бы не держал в кластере. В кластере он оставлен в тестовых целях. В кластере только fluentd и kibana**
- Плэйбук gitlab-configure.yml, в котором через api gitlab создаются
  - группа search-engine
  - три проекта: ui, crawler, deploy из репозиториев, обозначенных выше
  - pipeline trigger в проекте deploy, возвращается токен в виде переменной
  - переменные:
    - CI_REGISTRY - домен docker registry: необходимо для push/pull образов
    - CI_REGISTRY_USER - пользователь docker registry: необходимо для push/pull образов
    - CI_REGISTRY_PASSWORD - пароль docker registry: нобходимо для push/pull образов
    - HTTP_HTTPS - http или https: необходимо для формирования ссылки на GITLAB
    - CI_GITLAB_FQDN - полное доменное имя GITLAB: необходимо для формирования ссылки на GITLAB
    - CI_DNS_ZONE - DNS зона: необходима для формирования ссылки на production environment
    - CI_SEARCH_ENGINE_DEPLOY_TOKEN - Токен пайплайна проекта deploy: необходим для формирования запроса на триггер
    - CI_RMQ_USER - пользователь rabbitmq
    - CI_RMQ_PASSWORD - пароль rabbitmq
  - **реализовано через модуль curl, т.к. на момент реализации не было внятного модуля для gitlab api**
  - раннеры по количеству, указанному в переменных ansible.
- **не стал шифровать чувствительные данные, т.к. зашифрованные значения проверяющему не передать. В боевом проекте переменные с логинами и паролями были бы зашифрованы. Наверное :)**

### Для деплоя приложения так же используются хельм чарты отдельных сервисов и сборный чарт.
- Реализовано хранение секретов в kubernetes. Логин и пароль к rabbitmq.

### Описание CI/CD.
- На каждый коммит запускается build и test. Тестируется докер образ, собранный на предыдущем шаге.
- На каждый коммит в ветки кроме master деплоится приложение в отдельную среду для этой ветки.
- На каждый аннотированный тэг так же запускаеся build и test.
    - Тестируется докер образ, собранный на предыдущем шаге.
    - Далее на собранный образ ставится тэг из git.
    - Предполагается, что тэги ставятся в формате n.n.n, например, 1.2.3.
    - Из реджистри берётся образ другого микросервиса версии последнего по номеру тэга.
    - Далее эти версии передаются вместе с триггером в другой пайплайн с чартами. В чарты подставляются эти версии и происходит релиз в staging.
    - Далее вручную можно запустить релиз в production. Значения переданных переменных сохранятся, т.к. это тот же пайплайн.


## Как запустить.
Предполагается, что
  - на рабочей машине установлены
    - Python2.7
    - Git
    - GCloud
    - Terraform
    - Ansible
    - Kubectl
    - Helm client
  - в GCP создан проект и настроены Gcloud, Terraform, Ansible dynamic inventory.

1. Создать в директории `terraform` файл `terraform.tfvars` из `terraform.tfvars.example`. Заполнить переменные. Выполнить `terraform init && terraform apply --auto-approve=true`. В результате будет создан бакет для хранения tfstate.
2. Создать вручную DNS зону в GCP и делегировать на выданные NS сервера имеющийся домен. В названии зоны необходимо заменить . на -. Не добавил создание зоны в код, т.к. гугл каждый раз создаёт новые NS сервера. В данном случае не увидел смысла решать проблемы с api родительской зоны/регистратора.
3. Создать в директории `terraform/stage` файл `terraform.tfvars` из `terraform.tfvars.example`. Заполнить переменные. Заполнить название bucket в `backend.tf`. Рекомендуется создать новую пару SSH ключей. Выполнить `terraform init && terraform apply --auto-approve=true`. См. общее описание.
4. В директории `ansible` выполнить `pip install -r requirements.txt --user`. В результате будут установлены требуемые модули python.
5. В директории `ansible/environments/stage` создать `gce.ini` из `gce.ini.example` и заполнить необходимые переменные. Это данные для динамического инвентори.
6. В директории `ansible/environments/stage/group_vars` заполнить переменную `dns_zone`, `initial_gitlab_password`, `credentials_file`, `service_account_email`, `project_id` и выполнить `ansible-playbook playbooks/main.yml`. См. общее описание.
  - Prometheus доступен по ссылке `http://prometheus.{{ dns_zone }}`
  - Grafana доступна по ссылке `http://grafana.{{ dns_zone }}`
Пароль пользователя admin можно получить командой
```kubectl exec -it $(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}' | grep -m 1 grafana) env | grep GF_SECURITY_ADMIN_PASSWORD```
  - Kibana доступна по ссылке `http://kibana.{{ dns_zone }}`
7. Пройти по имени `https://gitlab.{{ dns_zone }}`, ввести root и заданный `initial_gitlab_password`. Создать в настройках access token с доступом к api и полученное значение внести в переменную `gitlab_token`. Выполнить `ansible-playbook playbooks/gitlab-configure.yml`. См. общее описание.
8. Далее можно запустить из веб-интерфейса гитлаба пайплайны микросервисов. Это создаст образы и запушит их в реджистри. В конце каждого пайплайна дёрнется основной пайплайн релиза в продакшн. Один раз он может завершиться со статусом failed. Это нормально. В результате должен развернуться релиз production в namespace production, доступный по ссылке `{{ dns_zone }}`.
9. Если создать ветку в любом из микросервисов и запушить изменения в гитлаб, будет создана среда review, которую можно удалить по кнопке.
10. Если запустить основной пайплайн вручную, то будет создана среда staging.
11. В интерфейсе Кибаны нужно создать индекс fluentd-* и можно смотреть логи.
